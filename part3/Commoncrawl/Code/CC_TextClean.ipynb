{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CC_TextClean.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "861LNY_OQA-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!sudo pip install -U nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1iSnrSpQIna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!python -m nltk.downloader all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLwBhJz3ZqkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U sacremoses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MkZ60tF2V6Rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sacremoses import MosesTokenizer, MosesDetokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOHj2bLdWTxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "detokenizer = MosesDetokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vuv6ssTIO3N9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/cc_raw_final.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euvqmfJaVgjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "# sentences = sent_tokenize(text)\n",
        "# print(sentences[2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XKl8Op2OWon",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "articles=text.split('\\n\\n\\n')\n",
        "print(articles[26500])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Aiy_MbpPJ_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f= open(\"filter_crawl_sports.txt\",\"a+\")\n",
        "for article in articles:\n",
        "  sentences = sent_tokenize(article)\n",
        "  for text in sentences:\n",
        "    words = text.split()\n",
        "    words=set(words)\n",
        "    if ('sport' in words):\n",
        "\n",
        "      words = [word for word in words if not word.startswith('#')]\n",
        "      words = [word for word in words if not word.startswith('@')]\n",
        "      words = [word for word in words if not word.startswith('http')]\n",
        "    # print(words[:100])\n",
        "      table = str.maketrans('', '', string.punctuation)\n",
        "      stripped = [w.translate(table) for w in words]\n",
        "      words = [word.lower() for word in stripped]\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "      lemmatized = [lemma.lemmatize(word) for word in words]\n",
        "      stg=detokenizer.detokenize(lemmatized, return_str=True)\n",
        "      f.write(stg+'\\n')  \n",
        "  f.write('\\n\\n')\n",
        "f.close()  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkhGsk2DWi34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f= open(\"filter_crawl_basketball.txt\",\"a+\")\n",
        "for article in articles:\n",
        "  sentences = sent_tokenize(article)\n",
        "  for text in sentences:\n",
        "    words = text.split()\n",
        "    words=set(words)\n",
        "    if ('basketball' in words):\n",
        "      words = [word for word in words if not word.startswith('#')]\n",
        "      words = [word for word in words if not word.startswith('@')]\n",
        "      words = [word for word in words if not word.startswith('http')]\n",
        "    # print(words[:100])\n",
        "      table = str.maketrans('', '', string.punctuation)\n",
        "      stripped = [w.translate(table) for w in words]\n",
        "      words = [word.lower() for word in stripped]\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "      lemmatized = [lemma.lemmatize(word) for word in words]\n",
        "      stg=detokenizer.detokenize(lemmatized, return_str=True)\n",
        "      f.write(stg+'\\n')  \n",
        "  f.write('\\n\\n')\n",
        "f.close()  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEccnXTbUJg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f= open(\"filter_crawl_baseball.txt\",\"a+\")\n",
        "for article in articles:\n",
        "  sentences = sent_tokenize(article)\n",
        "  for text in sentences:\n",
        "    words = text.split()\n",
        "    words=set(words)\n",
        "\n",
        "    if ('baseball' in words):\n",
        "      words = [word for word in words if not word.startswith('#')]\n",
        "      words = [word for word in words if not word.startswith('@')]\n",
        "      words = [word for word in words if not word.startswith('http')]\n",
        "    # print(words[:100])\n",
        "      table = str.maketrans('', '', string.punctuation)\n",
        "      stripped = [w.translate(table) for w in words]\n",
        "      words = [word.lower() for word in stripped]\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "      lemmatized = [lemma.lemmatize(word) for word in words]\n",
        "      stg=detokenizer.detokenize(lemmatized, return_str=True)\n",
        "      f.write(stg+'\\n')  \n",
        "  f.write('\\n\\n')\n",
        "f.close()  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QG1HV0V4UKV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f= open(\"filter_crawl_football.txt\",\"a+\")\n",
        "for article in articles:\n",
        "  sentences = sent_tokenize(article)\n",
        "  for text in sentences:\n",
        "    words = text.split()\n",
        "    words=set(words)\n",
        "\n",
        "    if ('football' in words):\n",
        "      words = [word for word in words if not word.startswith('#')]\n",
        "      words = [word for word in words if not word.startswith('@')]\n",
        "      words = [word for word in words if not word.startswith('http')]\n",
        "    # print(words[:100])\n",
        "      table = str.maketrans('', '', string.punctuation)\n",
        "      stripped = [w.translate(table) for w in words]\n",
        "      words = [word.lower() for word in stripped]\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "      lemmatized = [lemma.lemmatize(word) for word in words]\n",
        "      stg=detokenizer.detokenize(lemmatized, return_str=True)\n",
        "      f.write(stg+'\\n')  \n",
        "  f.write('\\n\\n')\n",
        "f.close()  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdwZUES_ULGu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f= open(\"filter_crawl_hockey.txt\",\"a+\")\n",
        "for article in articles:\n",
        "  sentences = sent_tokenize(article)\n",
        "  for text in sentences:\n",
        "    words = text.split()\n",
        "    words=set(words)\n",
        "\n",
        "    if ('hockey' in words):\n",
        "      words = [word for word in words if not word.startswith('#')]\n",
        "      words = [word for word in words if not word.startswith('@')]\n",
        "      words = [word for word in words if not word.startswith('http')]\n",
        "    # print(words[:100])\n",
        "      table = str.maketrans('', '', string.punctuation)\n",
        "      stripped = [w.translate(table) for w in words]\n",
        "      words = [word.lower() for word in stripped]\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "      lemmatized = [lemma.lemmatize(word) for word in words]\n",
        "      stg=detokenizer.detokenize(lemmatized, return_str=True)\n",
        "      f.write(stg+'\\n')  \n",
        "  f.write('\\n\\n')\n",
        "f.close()  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNNz5lL8YNAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}