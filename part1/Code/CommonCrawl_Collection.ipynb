{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CommonCrawl_Collection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wT5E3EDXx7-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install warc\n",
        "!pip install warc3-wet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmX3z2_H3rsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cr7JMsCxy4SV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir cc_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cajc2-Sk6fk_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf cc_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVLNtE48x30Z",
        "colab_type": "code",
        "outputId": "59323ad7-86ba-4465-a0b8-5fa2d4ae9dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import gzip\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import csv\n",
        "from glob import glob\n",
        "import warc\n",
        "import urllib\n",
        "import urllib.request\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "index_list = [\"2019-09\", \"2019-13\"]\n",
        "keywords = 'sports' or 'basketball' or 'baseball' or 'hockey' or 'football'\n",
        "\n",
        "def search_domain(domain):\n",
        "    record_list = []\n",
        "    \n",
        "    for index in index_list:\n",
        "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
        "        cc_url += \"url=%s/&filter=mime:text/html&limit=5&output=json\" % domain\n",
        "\n",
        "        response = requests.get(cc_url)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            records = response.content.splitlines()\n",
        "            for record in records:\n",
        "                record_list.append(json.loads(record))  \n",
        "    return record_list     \n",
        "\n",
        "def download_page(records):\n",
        "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
        "    fcc=open(\"cc_raw_final.txt\",'a')\n",
        "    c = 1\n",
        "    link_count = 0\n",
        "    for record in records:\n",
        "        offset, length = int(record['offset']), int(record['length'])\n",
        "        offset_end = offset + length - 1\n",
        "        dummy = record['filename'].replace('/warc/','/wet/').replace('.warc.','.warc.wet.')\n",
        "        link = prefix + dummy\n",
        "        holder = \"cc_data/\"+str(c) + \".warc.wet.gz\"\n",
        "        try:\n",
        "            if(urllib.request.urlretrieve(link,holder)):\n",
        "                c = c + 1\n",
        "            warc_files = glob(holder)\n",
        "\n",
        "            for fn in warc_files:\n",
        "                f = warc.open(fn)\n",
        "                for record in f:\n",
        "                    url = record.header.get('warc-target-uri', None)\n",
        "                    if not url:\n",
        "                        continue\n",
        "                    #else :\n",
        "                    text = record.payload.read()\n",
        "                    if(detect(text.decode(\"utf-8\")) == 'en'):\n",
        "                        if  keywords in text.decode(\"utf-8\").lower():\n",
        "                            fcc.write(str(text, 'utf-8'))\n",
        "                            fcc.write('\\n')\n",
        "                            fcc.write('\\n\\n\\n\\n\\n')\n",
        "                            link_count = link_count + 1\n",
        "        except Exception:\n",
        "            pass\n",
        "    fcc.close()\n",
        "    print(\"number of links parsed:\",link_count)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Starting CommonCrawl Search\")\n",
        "\n",
        "    domain = \"*.wikipedia.org\"\n",
        "    record_list = search_domain(domain)\n",
        "    demo = download_page(record_list)\n",
        "    print(\"all done:\", demo)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting CommonCrawl Search\n",
            "number of links parsed: 110\n",
            "all done: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Za4FuXsPx5Yq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}